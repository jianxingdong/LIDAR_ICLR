%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% LVF_iclr2015_v1.tex file
%%%% Authors: Michael Giering, Kishore Reddy, Vivek Venugopalan
%%%% adapted from iclr2015.tex
%%%% v1 12-15-2014 13:35:11
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{iclr2015,times} 
\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{fixltx2e} % Fixing numbering problem when using figure/table* 
\usepackage{gensymb}

\title{Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks}

\author{Michael Giering, Kishore Reddy, Vivek Venugopalan\\
Decision Support \& Machine Intelligence Group \\
United Technologies Research Center\\
E. Hartford, CT 06060, USA \\
Email: \{gierinmj, kkreddy, venugov\}@utrc.utc.com}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrconference
\begin{document}


\maketitle

\begin{abstract}

%When performing multi-modal fusion to perform an analytic task, spatio-temporal registration of the incoming signals is often a prerequisit to analyzing the %fused data and critical to the stability of the analysis. Lidar-Video systems like on those many driverless cars are a common example of where keeping the %Lidar and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of %heterogeneous data to detect the misalignment of the Lidar-video inputs. A number of variations were tested on the Ford LV driving test data set with %minimal tuning of the deep conv nets parameters. 

The ability to simlutaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle's physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisist to analyzing the fused data. The persistance and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LIDAR-Video systems like on those many driverless cars are a common example of where keeping the LIDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LIDAR-video inputs.  A number of variations were tested on the Ford LV driving test data set and will be discussed. To the best of our knowledge the use of multimodal deep convolutional neural networks for dynamic real-time LIDAR-video registration has not been presented.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation} % (fold)
\label{sec:motivation}
% please include other motivations
Navigation and situational awareness of optionally manned vehicles requires the integration of multiple sensing modalities such as LIght Detection and Ranging (LIDAR) and video, but could just as easily be extended to other modalities including RAdio Detection And Ranging (RADAR), Short-Wavelength Infrared (SWIR) and Global Positioning System (GPS). Spatio-temporal registration of information from multi-modal sensors is technically challenging in its own right. For many tasks such as pedestrian and object detection tasks that make use of multiple sensors, decision support methods rest on the assumption of proper registration. Most approaches \cite{Bodensteiner2012Real-time-} in LIDAR-video for instance, build separate vision and lidar feature extraction methods and identify common anchor points in both. Alternatively, by generating a single feature set on LIDAR, Video and optical flow, it enables the system to to capture mutual information among modalities more efficiently. The ability to dynamically register information from the available data channels for perception related tasks can alleviate the need for anchor points \emph{between} sensor modalities. We see auto-registration as a prerequisit need for operating on multi-modal information with confidence.

%better fusion
Deep neural networks lend themselves in a seamless manner for data fusion on time series data. It has been shown [Ng multimodal] for some challenges in which the modalities share significant mutual information, the features generated on the fused information can provide insight that neither input alone can. In effect the ML version of, "the whole is greater than the sum of it's parts". 

%speed constraints
Speed constraints of real time navigation also constrain model selection. The trained nnets easily run within the real-time constraints of common frame rates and lidar data collection.

%Applied research perspective
From an applied research perspective, it is possible to create such systems with far less overhead. The need for domain experts and hand-crafted feature design are lessened, thereby allowing more rapid prototyping and testing. 

%%probably in next steps
The generalization of autoregistration across multiple assets is clearly a path to be explored. 

%%dynamics
By including optical flow as input channels, we imbue the nnet with information on the dynamics observed across time steps. 

% section motivation (end)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Previous Work} % (fold)
\label{sec:previous_work}
%% image registration
Kishore here

%%Multimodal data fusion
A great amount has been published on various multimodal fusion methods. The most common approaches taken generate features of interest in each modality seperately and create a decision support mechanism that aggregates features across modalities. If spatial alignment is required across modalities, as it is for LIDAR-video such filter methods \cite{Thrun2011Googles-dr} are required to ensure proper inter-modal registration. These filter methods for leveraging 3D LiDAR and 2D images are often geometric in nature and make use of projections between the different data spaces. 



%%DL multimodal data fusion with CNN's

The use of deep neural networks to analyze multimodal sensor inputs  has increased sharply in the last couple of years, including audiR-video \cite{Ngiam2011Multimodal} \cite{Kim2013Deep-Learn}, image/text \cite{Srivastava2012Multimodal}, image/depth \cite{Lenz2013Deep-Learn} and LIDAR-video. To the best of our knowledge the use of multimodal deep neural networks for dynamic real time LIDAR-video registration has not been presented.

A common question often arises in data fusion methods, which is "at what level should features from the differing sensor streams be brought together?" Most similar to the more traditional data fusion methods is to train DNN's independently on sensor modalities and then use the high-level outputs of those networks as inputs to a subsequent DNN. This is analogous to the earlier example of learning 3D/2D features and subsequently identifying common geometric features. 

It is possible however to apply DNN's with a more agnostic view enabling a unified set of features to be learned across multimodal data. In these cases the input channels aren't differentiated. Unsupervised methods in particular applying DBM's for learning such joint representations have been successful.  

DCNN's enable a similar agnostic approach to input channels. A significant difference is that target data is required to train them as classifiers. \emph{find examples in the literature}. This is the approach chosen by us for automating the registration of LIDAR-video and optical-flow, in which we are combining 1D/3D/2D data representations to learn a unified model across as many as 6D. 


% section previous_work (end)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Statement} % (fold)
\label{sec:problem_statement}

Being able to detect and correct the misalignment (registration, calibration) among sensors of the same or different kinds is critical when operating on the fused information emanating from them. For this work Deep Convolutional Neural Networks (DCNN) were implemented for the detection of small spatial misalignments in LIDAR and Video frames. The data collected from a driverless car was chosen as the multi-modal fusion test case. LIDAR-Video is a common combination for providing perception capabilities to many types of ground and airborne platforms including driverless cars \cite{Thrun2011Googles-dr}. 

\subsection{Ford LIDAR-Video Dataset and Experimental Setup} % (fold)
\label{sub:ford_lidar_video_dataset_and_experimental_setup}

\begin{figure}[htbp]
    \centering
        \includegraphics[scale=0.45]{Figures/ford-truck-sensors-final.png}
    \caption{Left: The modified Ford F-250 pickup truck. Right: Sample image from front facing camera and green dots indicate the region of lidar data.}
    \label{fig:ford-truck-sensors}
\end{figure}

The FORD LIDAR-Video dataset \cite{Pandey2011Ford-campu} is collected by an autonomous Ford F-250 vehicle integrated with the following perception and navigation sensors as shown in Figure \ref{fig:ford-truck-sensors}:
\begin{itemize}
    \item Velodyne HDL-64E LIDAR with two blocks of lasers spinning at 10 Hz and a maximum range of 120m.
    \item Point Grey Ladybug3 omnidirectional camera system with six 2-Megapixel cameras collecting video data at 8fps with $1600\times1600$ resolution.
    \item Two Riegl LMS-Q120 LIDAR sensors installed in the front of the vehicle generating range and intensity data when the laser sweeps its 80\degree field of view (FOV).
    \item Applanix POS-LV420 INS with Trimble GPS system providing the 6 degrees of freedom (DOF) estimates at 100 Hz.
    \item Xsens MTi-G sensor consisting of accelerometer, gyroscope, magnetometer, integrated GPS receiver, static pressure sensor and temperature sensor. It measures the GPS co-ordinates of the vehicle and also provides the 3D velocity and 3D rate of turn.
\end{itemize}

\begin{figure}[htbp]
    \centering
        \includegraphics[scale=0.35]{Figures/ford_train_test_track.jpg}
    \caption{Training (A to B) and testing (C to D) tracks in the downtown Dearborn Michigan.}
    \label{fig:ford_train_test_track}
\end{figure}

This dataset is generated by the vehicle while driving in and around the Ford research campus and downtown Michigan. The data includes feature rich downtown areas as well as featureless empty parking lots. As shown in Figure \ref{fig:ford_train_test_track}, we divided the data set into training and testing sections A to B and C to D respectively. They were chosen in a manner that minimizes the likelihood of contamination between training and testing. Because of this, the direction of the light source is never the same in the testing and training sets. %If our methods are generalizable, they should be able to overcome this bias in the data.   

% subsection ford_lidar_video_dataset_and_experimental_setup (end)

\subsection{Optical Flow} % (fold)
\label{sub:optical_flow}

\begin{figure}[htbp]
    \centering
        \includegraphics[scale=0.85]{Figures/OpticalFlow_example_final.png}
    \caption{Optical flow: Hue indicates orientation and saturation indicates magnitude}
    \label{fig:Figures_OptFlow placeholder}
\end{figure}

In the area of navigation of mobile robots, optical flow has been widely used to estimate egomotion [\cite{Prazdny1980-egomotion-OF}], depth maps [\cite{Shahraray1988-depthestimation-OF}], reconstruct dynamic 3D scene depth [\cite{Yang2012-reconstruction-OF}], and segment moving objects [\cite{Shao2002-seg-OF}]. Optical flow gives an estimate of velocity at each pixel from two consecutive frames, denoted by $\vec{u}$ and $\vec{v}$. The motion field from these two frames is measured by the motion of the pixel brightness pattern, where the changes in image brightness is due to the camera or object motion. \cite{Liu2009Beyond-Pix} describes an algorithm for computing optical flow from images, which is used during the preprocessing step. Figure \ref{fig:Figures_OptFlow placeholder} shows an example of the optical flow computed using two consecutive frames from the Ford LIDAR-video dataset.


% In our work we use optical flow as an additional channel derived from two consecutive frames to capture the dynamics. Optical flow gives an estimate of velocity at each pixel given two consecutive frames. \cite{Chase2008Real-Time-} demonstrated that optical flow can be performed in real-time on FPGA and GPU architectures. In our work, we used the algorithm described in [\cite{Liu2009Beyond-Pix}] for computing optical flow. Figure \ref{fig:Figures_OptFlow placeholder} shows an example of the optical flow obtained using two frames from Ford LV dataset.

% subsection optical_flow (end)

\subsection{Preprocessing} % (fold)
\label{sub:preprocessing}
%image of LVF data with channels, patches and images(frames) denoted.
At each video frame timestep, the inputs to our model consist of \emph{C} channels of data with \emph{C} ranging from 3-6 channels. Channels consist of grayscale \emph{Gr} or \emph{(R,G,B)} information from the video, horizontal and vertical components of optical flow \emph{(U,V)} and depth information \emph{L} from LIDAR. The data from each modality is reshaped to a fixed size of $800\times256$ values, which are partitioned into $p\times p$ patches at a prescribed stride as shown in Figure \ref{fig:ImageChStride}. Each patch $p\times p$ is stacked across \emph{C} channels, effectively generating a vector of \emph{C} dimensions. The different preprocessing parameters are denoted by patch size \emph{p}, stride \emph{s} and the number of input channels \emph{C}.

\begin{figure}[htbp]
    \centering
        \includegraphics[scale=0.65]{Figures/ImagePatchChannel.pdf}
    \caption{Stacking of $p\times p$ patch over all the different channels}
    \label{fig:ImageChStride}
\end{figure}

% Each channel was cropped to a uniform 800x256 pixels. Each time step has an 800 x 256 x C array of integer values.
%
% These arrays were subdivided into p x p x C patches at a prescribed stride as shown in Figure \ref{fig:ImageChStride}. For any experiment we can denote the preprocessing parameters
% \begin{itemize}
% \item R,G,B --- Frame color channels.
% \item W --- Grey color channel.
% \item U,V --- optical flow channels.
% \item L --- lidar depth channel.
% \item C --- number of input channels.
% \item p --- patch size.
% \item s --- stride.
% \end{itemize}
%
% For a given frame of size 800 x h there are approximately n= (800 x h)/s patches (exact number?). The training and test sets had X and Y frames respectively, therefore the entire training data set consists of  N = n x X inputs of the patch-size dimension.

Preprocessing is repeated \emph{N} times, where \emph{N} is the number of offset classes. Figure \ref{fig:Figures_Ellipse} shows a 9 class elliptically distributed set of offsets. For each offset class, the video (R,G,B) and optical flow (U,V) channels are kept static and the depth (L) channel from the lidar is moved by the offset simulating a misalignment between the video and the lidar sensors. 

%At a given timestep, all n patches extracted from the C-channels for different offsets are grouped in to different classes for training purposes. The information from each channel for a given patch location are vectorised into a d=p x p x C dimension vector resulting in a nxd dimension matrix for each offset class at a given timestep.  
%\textbf{Kishore explain how you generated the data.} 
%figure showing offset classes. 

\begin{figure}[htbp]
    \centering
        \includegraphics[scale=0.85]{Figures/ellipse.png}
    \caption{Visualization of the elliptically distributed 9 classes}
    \label{fig:Figures_Ellipse}
\end{figure}

%% image of Ford Data set marked with Testing and Training


In order to accurately detect misalignment in the LIDAR and Video sensor data, we've assumed there needs to be a lower bound on the amount of information present in each channel. For this data set, L was the only channel with regions of low information. A preprocess step was to eliminate all patches corresponding to L data with variance less than a threshold (\textless x). This leads to the elimination of the majority of foreground patches in the data set, reducing the size of the training set by approximately 80\%. In the testing stage, the same threshold x is used to not perform classification on patches with less information.
%\textbf{z pct KISHORE}

% subsection preprocessing (end)

% section problem_statement (end)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Description} % (fold)
\label{sec:model_description}

\textbf{need to describe the parameters post-processing,classification metric for each patch,a table with common params for the experiments would help,voting scheme}

Our models for auto-registration are DCNN's trained to classify the current misalignment of the LIDAR-video data streams into one of a predefined set of offsets. DCNN's are probably the most successful deep learning model to date on fielded applications. The fact that the algorithm shares weights in the training phase, results in fewer model parameters and more efficient training. DCNN's are particularly useful for problems in which local structure is important, such as object recognition in images and temporal information for voice recognition. The alternating steps of convolution and pooling \textbf{(as depicted in figure X)}  generates features at multiple scales which in turn imbues DCNN's with scale invariant characteristics.


The model consists of a 4-layer \textbf{?} CNN classifier \textit{see image of network} that estimates the offset between the LV inputs at each time step. For each patch within a timestep, there are O variants with the LVF inputs offset by the predetermined amounts. The CNN outputs to a softmax layer, thereby providing an offset classification value for each patch of the frame. 
% figure of the colored class choices for the 5 class data set
figure x: In the 5 class example we color each patch of the frame with a color corresponding to the predicted class. 

For each frame a simple voting scheme is used to aggregate the patch level offset predictions to frame level predictions. A sample histogram of the patch level predictions is show in figure x.


% section model_description (end)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments and Post-processing} % (fold)
\label{sec:experiments_and_post_processing}

The NVIDIA Kepler series K40 GPUs [\cite{NVIDIA-Inc.2012NVIDIAs-Ne}] are very FLOPS/Watt efficient and are being used to drive real-time image processing capabilities [\cite{Venugopal2013Accelerati}]. These GPUs consist of 2880 cores with 12 GB of on-board device memory (RAM). Deep Learning applications have been targeted on GPUs previously in [\cite{Krizhevsky2012Imagenet-C}] and these implementations are memory bound. A GPU with higher memory capacity is excellent for these experiments due to the number of channels that are stacked and provided as the input to the DCNN. The LIDAR-Video dataset is augmented with the optical flow information resulting in 6 channels which means that the input to the DCNN is $32\times32\times6$ per patch. Figure \ref{fig:Figures_lidar_dcnn_setup1} shows the setup used for the DCNN implementation on the GPU, where \emph{C} defines the number of channels.

\begin{figure}[htbp]
    \centering
        \includegraphics[scale=0.35]{Figures/lidar_dcnn_setup1.pdf}
    \caption{Experimental setup of the LIDAR-Video DCNN with $5\times5$ convolution}
    \label{fig:Figures_lidar_dcnn_setup1}
\end{figure}

\textit{Need a complete list of the experiments run
images to visualize the frame level results
please place any confusion matrices and your comments on what you think the results say.
feel free to suggest any tables or other visuals to include.}




\subsection{5 class tests} % (fold)
\label{sub:5_class_tests}
% require a table of the different parameter settings, along with some aggregate measures from the confusion matrices. 
In our initial tests, the linearly distributed set of 5 offsets of the LV data were performed. Table 1 lists the inputs and CNN parameters explored ranked in the order of increasing accuracy \textbf{(define accuracy and other cm metrics), include training vs test error and conf mats if room allows}.  

As can be seen ... 
% subsection 5_class_tests (end)


\subsection{9 class tests} % (fold)
\label{sub:9_class_tests}
% require a table of the different parameter settings, along with some aggregate measures from the confusion matrices. 
The subsequent tests were designed to understand whether the simple linear displacement model of the 5-class test could be generalized to a model capable of discriminating multiple directions and displacement magnitude. To achieve this 8 positions were chosen on an ellipse along with it's center \textbf{describe the parabola}. LV was offset in a manner similar to the 5 class test. Nine training and test sets were generated and an identical patch level CNN was constructed differing only in the 9 class softmax output layer. 

\begin{figure}[htbp]
    \centering
        \includegraphics[scale=0.85]{Figures/Voting_5class.jpg}
    \caption{Placeholder: Voting}
    \label{fig:Figures_Voting}
\end{figure}

Table 2 lists the inputs and CNN parameters explored ranked in the order of increasing accuracy \textbf{(define accuracy and other cm metrics), include training vs test error and conf mats if room allows}.  

 \textbf{Discussion: what results confirmed expectations or surprised us (grey scale). Can we confidently say optical flow improves prediction. }
% subsection 9_class_tests (end)

% section experiments_and_post_processing (end)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work} % (fold)
\label{sec:conclusions_and_future_work}
We did it. We're great.

%future: 
The next step in taking this work forward is to complete our development of a deep auto-registration method for ground and airborn platforms requiring no apriori calibration ground truth.  Our airborne applications in particular present noiser data with an increased number of degrees of freedom. The extension of these methods to simultaneously register information across multiple platforms and larger numbers of modalities will provide interesting challenges that we look forward to working on. 

% section conclusions_and_future_work (end)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{references}

\bibliographystyle{iclr2015}
\bibliography{references}

\end{document}
