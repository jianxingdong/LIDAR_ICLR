\documentclass{article}
\usepackage{spconf,amsmath,epsfig}
\usepackage{bm}

% Title.
% ------
\title{Dynamic Heterogeneous Sensor Registration for Vehicle Perception via Deep Neural Networks}

\name{Michael Giering, Vivek Venugopalan, Kishore Reddy}

\address{United Technologies Research Center\\
	 Decision Support and Machine Intelligence Group\\
	 411 Silver Lane, East Hartford CT 06108}


\begin{document}

% {\tt{\textbackslash}iclrconference}

\maketitle

\begin{abstract}
Test ...When performing multi-modal fusion to perform an analytic task, spatio-temporal alignment of the incoming signals is often a key to the stability of all methods subsequently applied to the fused data. Lidar-Video systems like those many driverless cars are a common example of where keeping the Lidar and video channels registered to common physical features is important. We develop a deep learning technique that takes multiple channels of heterogeneous data, to test whether we can detect misalignment of the Lidar-video inputs.  A number of variations were tested on the Ford LV driving test data set. 

\end{abstract}

\section{Motivation}
% please include other motivations
Navigation and situational awareness of optionally manned vehicles requires the integration of multiple sensing modalities such as LIDAR and video, but could just as easily be extended to  Radar, SWIR and GPS. Spatio-temporal registration of information from multi-modal sensors is technically challenging in its own right. For many tasks such as pedestrian and other object detection tasks that make use of multiple sensors, decision support methods rest on the assumption of proper registration. Most approaches in LIDAR-video for instance, build separate vision and lidar feature extraction methods and then try to identify common anchor points in both. The ability to dynamically register information from the available data channels for perception related tasks can alleviate this achielles ankel of multi-modal sensor fusion.

%better fusion
Deep neural networks lend themselves in a seamless manner for data fusion on these types of problems. By building a single model on all information being collected at once, it has been shown that features generated on the fused information [][] can provide insight that neither input alone can. From a more applied perspective, it is possible to create such systems with far less overhead. The need for domain experts and hand-crafted feature design are lessened, thereby allowing more rapid prototyping and testing. 

%speed constraints
The trained nnets easily run within the real-time constraints of common frame rates and lidar data collection. 

%probably in next steps
The generalization of autoregistration across multiple assets is clearly a path to be explored. 

%dynamics
By including dynamic flow as input channels, we imbue the nnet with information on the dynamics observed across time steps. 


\section{Previous Work}
Need some references to define the state of the art 

\section{Problem Statement}
need detailed description and citation regarding the ford data set
need greater clarity on the optical flow method used. 

Being able to detect and correct the misalignment among sensors of the same or different kinds is critical when operating on the fused information emanating from the sensors. For this work DCNN's were implemented for the detection of small spatial misalignments in Lidar and Video frames. The data was collected from a driverless car was chosen as the multi-modal fusion test case. LV is a common combination for providing perception capabilities to many types of ground and airborne robots including driverless cars [google, ford]. 

\subsection{Ford LV data set and experimental setup}
\textbf{ Kishore -Detailed description of the ford data set [], our test and training and the justifications for it. framerate. 
Vivek - a brief description of the hardware used. }
%% image the ford data collection system and camera views
%% image of Ford Data set marked with Testing and Training
As shown in diagram-n, we divided the data set into training and testing sections A and B. They were chosen in a manner that minimizes the likelihood of contamination between training and testing. Because of this, the direction of the lighting is source is never the same in the testing and training sets. This provides an additional measure of testing the generalizability of our models. 

\subsection{Preprocessing}
%image of LVF data with channels, patches and images(frames) denoted.
At each video frame timestep, the inputs to our model consisted of C-channels of data with C ranging from 3-6 channels. Channels consisted of inputs that included greyscale and (R,G,B)-video channels, horizontal and vertical optical flow and Lidar depth information. Each channel was cropped to a uniform 800x??? pixels. Each time step has an 800 x ??? x C array of integer values.

These arrays were subdivided into p x p x C patches at a prescribed stride. For any experiment we can denote the preprocessing parameters 
\begin{itemize}
\item R,G,B --- Frame color channels.
\item U,V --- optical flow channels.
\item L --- lidar depth channel.
\item C --- number of input channels.
\item p --- patch size.
\item s --- stride.
\end{itemize}  

For a given frame of size 800 x h there are approximately n= (800 x h)/s patches (exact number?). The training and test sets had X and Y frames respectively, therefore the entire data set consists of  N = n x X inputs of the patch-size dimension. 

Preprocessing is repeated O times, where O is the number of offset classes. For this work we used two setups. A 5 class, linearly distributed set of offsets and a 9 class eliptically distributed set of offsets. (see figure x) For each offset class, \textbf{Kishore explain how you generated the data.} 
%figure showing offset classes. 

In order to accurately detect misalignment in the LV sensor data, we've assumed there needs to be a lower bound on the amount of information present in each channel. For this data set, L was the only channel with regions of low information. A preprocess step was to eliminate all patches corresponding to L data with variance <x. This leads to the elimination of the majority of foreground patches in the data set, reducing the size of the training set by \textbf{z pct KISHORE}

\section{Model Description}
need to describe the parameters post-processing
classification metric for each patch
a table with common params for the experiments would help
voting scheme

The model consists of a 4-layer \itembf{?} CNN classifier \textit{see image of network} that estimates the offset between the LV inputs at each time step. For each patch within a timestep, there are O variants with the LVF inputs offset by the predetermined amounts. The CNN outputs to a softmax layer, thereby providing an offset classification value for each patch of the frame. 
% figure of the colored class choices for the 5 class data set
figure x: In the 5 class example we color each patch of the frame with a color corresponding to the predicted class. 

For each frame 

\section{experiments and post-processing}
Need a complete list of the experiments run
images to visualize the frame level results
please place any confusion matrices and your comments on what you think the results say.
feel free to suggest any tables or other visuals to include.

\section{conclusion and future work}

\section{references}
populate the papers to be cited in the folder and if possible the bib file


%%
%%Nothing below here is relevant for ICLR yet
%%

We propose to utilize Deep Convolutional Neural Networks (DCNNs) that have multiple
hidden layers for feature extraction and task discrimination enabling perception and decision support for autonomous vehicles. \cite{Verma2013} , \cite{Tran2013}. The inputs to the model may be from multiple sensors of the same or different kind as well as other non-sensor information. Examples of sensor modalities could include Radar, LIDAR, video, SWIR and GPS.

DCNN's  have recently surpassed other state of the art methods in image analysis, audio analysis and other domains. DCNN's are a specific type of \emph{deep neural networks (DNNs)} \cite{HintonSPS2012}\cite{HintonDNN} that are able to take advantage of local structure in data. These are high dimensional encodings that preserve most information and hence are excellent for tasks such as structure detection and disambiguation.  

A DNN is a feedforward artificial neural network that has more than one layer of hidden units
between its inputs and outputs. Each hidden unit, $j$, uses the
nonlinear mapping function, often the logistic function, to map its total input from the layer below, $x_j$,
to the scalar state, $y_j$, that it sends to the layer above,
\[
y_j = \frac{1}{1+e^{-x_j}}, \quad x_j = b_j+\sum_iy_iw_{ij},
\]
where $b_j$ is the bias of unit $j$, $i$ is an index over units in the
layer below, and $w_{ij}$ is the weight to unit $j$ from unit $i$ in
the layer below. 

For CNN's convolution is performed at the convolutionsal layers to extract local structure features from the features of the previous layer. Additive bias is applied at this point. This is followed by a local pooling step. A nonlinear mapping (most often a sigmoid) is applied after either the convolution or pooling layer and varies by practioner.  Iteratively repeating these convolution and pooling steps results in a CNN architecture \cite{2013ImageClassification} \cite{LeCunCNN2}.

\emph{Image ofD CNN}

The value for each spatial point (\emph{x,y}) on the \emph{j}th feature map in the \emph{i}th layer denoted as $v^{xy}_{ij}$ is

\[{v^{xy}_{ij}}=tanh(b_{ij}+\sum_m\sum_{p=0}^{P_{i}-1}\sum_{q=0}^{Q_{i}-1}w^{pq}_{ijm}v^{(x+p)(y+q)}_{(i-1)m}),
\]
 
where $b_{ij}$ is the bias for this feature map, m indexes over the set of feature maps in the (i-1)th layer connected to the current feature map, $w^{pq}_{ijm}$ is the value at the position \emph{(p,q)} of the kernel connected to the \emph{k}th featuremap, and \emph{$P_i$} and \emph{$Q_i$} are the height and width of the kernel respectively. 

The target layer of the DCNN can be chosen to represent the degree to which sensor information is misaligned. This infomation can in turn be used to properly register sensor data by physical manipulation of the sensors or within the system software. 

Methods for improving the accuracy of DCNN's such as dropout may be applied. It is especially useful for applications where the amount of available data is of marginal size to learn the number of necessary parameters in the DCNN. Most consider it analogous to averaging the results over multiple lower dimensional versions of the model. 

\emph{With ground truth}
In the event that you have initial data with sensors properly registered, a DCNN can be trained on versions of the data at various known offsets. The input to this model are a matrix representation of two sensors and any supplemental information nodes. (E.g. LIDAR - Video - Optical Flow ) A DCNN is created with the standard iterative layers of convolution and pooling, terminating in a soft-max layer for classification of any input sensor pairings as one of the known offsets. The softmax layer enables the user to interpret the offset prediction as a distribution or as a discrete classification.

\emph{With NO ground truth}
Given multi-modal sensor data with no prior knowledge of the system, it is often still possible to register their data streams. For illustration we use the LIDAR-video registration example. Creating an overconstrained DAC (Deep auto-encoder) DEFINE, we can drive the autoencoder to capture mutual information in both the LIDAR and video by reducing the rand of the DAC bottleneck layer well beyond the rank at which optimal reconstruction occurs. Minimizing the reconstruction error with respect to relative shifts of the LIDAR video data reflects that the current alignment of the sensor data has the greatest correlation possible (smallest misalignment). This method can be applied for both spatial and temporal registration. 


\section{Benefits of the Invention}
As the number of channels and modalities of information increase modeling perception systems becomes difficult if not impossible in part due to the large overhead of creating and operating registration methods, especially for real time streaming applications. This invention removes the need for timely expert based feature creation and implicitly generates expressive data features which have been demonstrated to be state of the art in machine learning []. In addition, once trained DCNN's are extremely fast with low memory and computational requirements, making them ideal for real time processing in energy constrained embedded systems.  The large volumes
of data typically collected in perception applications is very suitable for training the large numbers of parameters deep neural networks such as this have.  

DNN's have been shown [] to be able to make more effective use of the information present in the data for discriminative tasks.

\section{Current Patent Strategy}
We will move forward capturing Deep Learning IP under UTRC. This is in response to very active IP filing in this domain, slow  response of BU's to adopt and the likelyhood of being able to license this technology for noncompetitive applications.

A less inclusive submission on Deep Learning for accelerometers has been chosen for patent application by Sikorsky. 

The use of deep learning methodologies for feature generation and event detection for PHM is unclaimed IP as far as we know at this time. 

The IP landscape in the area of deep neural networks is extremely active. Time is of the essence in protecting this IP if deemed in the interest of UTC. 

\section{Licensing Opportunities}
\begin{itemize}
\item  Any multi-sensor perception or PHM application
\item  dynamic registration of multiple sensors (UAV's, satelites) and modalities (multispectral cameras, audio, high frequency time series)
\item  ability to properly integrate sensor information as they appear or disappear. 
\item autoregistration of imagery from various platforms across differing spectral bands. 
\item ability to dynamically temporally shift sensor data to remove time inherent lags between sensors. e.g. Multiple microphones receiving sounds from a single event at different times. 
\end{itemize}

\bibliographystyle{IEEEbib}
\bibliography{references}

\end{document}
